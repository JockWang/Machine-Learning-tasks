# 第4章 决策树

![img](https://uploader.shimo.im/f/gN57ExgV90IGwAK0.png!thumbnail)

[TOC]

## 树的划分流程

一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点；叶结点对应于决策结果，其他每个结点则对应于一个属性测试；每个结点包含的样本集合根据属性测试的效果被划分到子结点中，根结点包含样本全集。

决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循简单且直观的“分而治之”策略。

决策树的生成是一个递归过程。在决策树基本算法中，有三种情形会导致递归返回：

* 当前结点包含的样本全属于同一类别，无需划分
* 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
* 当前结点包含的样本集合为空，不能划分

## 树如何划分

决策树学习的关键在于如何选择最优划分属性。我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”越来越高。

**信息熵**（information entropy）是度量样本集合纯度最常用的一种指标。假定当前样本集合$D$中的第$k$类样本所占的比例为$p_k(k=1,2,...,|y|)$，则$D$的信息熵定义为
$$
Ent(D) = - \sum_{k=1}^{|y|}p_klog_2p_k
$$
其中，$Ent(D)$的值越小，则D的纯度越高。**$Ent(D)$的最小值为0，最大值为$log_2|y|。$**

属性$a$对样本$D$进行划分所获得的“**信息增益**”（information gain）。$ID3$决策树算法就是使用信息增益为准则选择划分属性的。
$$
\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)
$$
信息增益越大，则意味着使用属性$a$来进行划分所获得的纯度提升越大。

实际上，**信息增益对可取值数目较多的属性有所偏好**，为了减少这种偏好可能带来的不利影响，决策树算法$C4.5$并没有直接使用信息增益，而是使用**增益率**（gain ratio）来选择最优划分属性。增益率的定义为：
$$
(D, a)=\frac{\operatorname{Gain}(D, a)}{\operatorname{IV}(a)}
$$
其中，
$$
\mathrm{IV}(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log _{2} \frac{\left|D^{v}\right|}{|D|}
$$
$IV(a)$属性$a$的“固有值”（intrinsic value）。属性$a$的可能取值数目越大，则$IV(a)$的值通常会越大。事实上，增益率准则对可取值数目较少的属性有所偏好。所以，$C4.5$算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：**先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

$CART$决策树使用**基尼指数**（Gini index）来选择划分属性。数据集$D$的纯度可用基尼值来度量：
$$
\begin{aligned} \operatorname{Gini}(D) &=\sum_{k=1}^{|\mathcal{Y}|} \sum_{k^{\prime} \neq k} p_{k} p_{k^{\prime}} \\ &=1-\sum_{k=1}^{|\mathcal{Y}|} p_{k}^{2} \end{aligned}
$$
$Gini(D)$反映了从数据集$D$中随机抽取两个样本，其类别标记不一致的概率，$Gini(D)$越小，则数据集$D$的纯度越高。

属性$a$的基尼指数定义为：
$$
\operatorname{Gini} \operatorname{index}(D, a)=\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Gini}\left(D^{v}\right)
$$
在候选属性集合$A$中，来选择那个使得划分后基尼指数最小的属性作为最优划分属性。

## 对抗过拟合的手段

**剪枝**（pruning）是决策树学习算法对付“过拟合”的主要手段。由于划分过程不断重复，导致决策树分支过多，出现过拟合。所以，通过主动去掉一些分支来降低过拟合的风险。

决策树剪枝的基本策略有“**预剪枝**”和“**后剪枝**”。

* 预剪枝是指决策树生成过程中，对每个结点在花分钱进行估计，若当前结点的划分不能带来巨册书泛化性能提升，则停止划分并将当前结点标记为叶结点；
* 后剪枝是指先从训练集生成一棵完整的决策树，然后自底向上对非叶结点进行考察，若将该结点对应的子树对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。



## 如何处理连续值、缺失值

## 各种决策树之间的比较

## 随机森林

