# 第4章 决策树

![img](https://uploader.shimo.im/f/gN57ExgV90IGwAK0.png!thumbnail)

[TOC]

## 树的划分流程

一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点；叶结点对应于决策结果，其他每个结点则对应于一个属性测试；每个结点包含的样本集合根据属性测试的效果被划分到子结点中，根结点包含样本全集。

决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循简单且直观的“分而治之”策略。

决策树的生成是一个递归过程。在决策树基本算法中，有三种情形会导致递归返回：

* 当前结点包含的样本全属于同一类别，无需划分
* 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
* 当前结点包含的样本集合为空，不能划分

## 树如何划分

决策树学习的关键在于如何选择最优划分属性。我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”越来越高。

**信息熵**（information entropy）是度量样本集合纯度最常用的一种指标。假定当前样本集合$D$中的第$k$类样本所占的比例为$p_k(k=1,2,...,|y|)$，则$D$的信息熵定义为
$$
Ent(D) = - \sum_{k=1}^{|y|}p_klog_2p_k
$$
其中，$Ent(D)$的值越小，则D的纯度越高。**$Ent(D)$的最小值为0，最大值为$log_2|y|。$**

属性$a$对样本$D$进行划分所获得的“**信息增益**”（information gain）。$ID3$决策树算法就是使用信息增益为准则选择划分属性的。
$$
\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)
$$
信息增益越大，则意味着使用属性$a$来进行划分所获得的纯度提升越大。

实际上，**信息增益对可取值数目较多的属性有所偏好**，为了减少这种偏好可能带来的不利影响，决策树算法$C4.5$并没有直接使用信息增益，而是使用**增益率**（gain ratio）来选择最优划分属性。增益率的定义为：
$$
(D, a)=\frac{\operatorname{Gain}(D, a)}{\operatorname{IV}(a)}
$$
其中，
$$
\mathrm{IV}(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log _{2} \frac{\left|D^{v}\right|}{|D|}
$$
$IV(a)$属性$a$的“固有值”（intrinsic value）。属性$a$的可能取值数目越大，则$IV(a)$的值通常会越大。事实上，增益率准则对可取值数目较少的属性有所偏好。所以，$C4.5$算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：**先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

$CART$决策树使用**基尼指数**（Gini index）来选择划分属性。数据集$D$的纯度可用基尼值来度量：
$$
\begin{aligned} \operatorname{Gini}(D) &=\sum_{k=1}^{|\mathcal{Y}|} \sum_{k^{\prime} \neq k} p_{k} p_{k^{\prime}} \\ &=1-\sum_{k=1}^{|\mathcal{Y}|} p_{k}^{2} \end{aligned}
$$
$Gini(D)$反映了从数据集$D$中随机抽取两个样本，其类别标记不一致的概率，$Gini(D)$越小，则数据集$D$的纯度越高。

属性$a$的基尼指数定义为：
$$
\operatorname{Gini} \operatorname{index}(D, a)=\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Gini}\left(D^{v}\right)
$$
在候选属性集合$A$中，来选择那个使得划分后基尼指数最小的属性作为最优划分属性。

## 对抗过拟合的手段

**剪枝**（pruning）是决策树学习算法对付“过拟合”的主要手段。由于划分过程不断重复，导致决策树分支过多，出现过拟合。所以，通过主动去掉一些分支来降低过拟合的风险。

决策树剪枝的基本策略有“**预剪枝**”和“**后剪枝**”。

* 预剪枝是指决策树生成过程中，对每个结点在花分钱进行估计，若当前结点的划分不能带来巨册书泛化性能提升，则停止划分并将当前结点标记为叶结点；
* 后剪枝是指先从训练集生成一棵完整的决策树，然后自底向上对非叶结点进行考察，若将该结点对应的子树对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。

预剪枝基于贪心本质禁止一些分支展开，给剪枝决策树带来了欠拟合的风险。

后剪枝决策树通常比预剪枝决策树保留了更多分支，欠拟合风险很小，泛化能力往往优于预剪枝决策树，但其训练时间开销比未剪枝决策树和预剪枝决策树都要大很多。

## 如何处理连续值、缺失值

### 连续值处理

$C4.5$决策树算法采用二分法对连续属性进行处理。假定$a$在$D$上出现了$n$个不同的取值，将这些值从小到大排序，基于划分点将$D$分为两个子集。

需要注意的是，与离散属性不同，若当前结点划分属性为连续属性，该属性还可以作为其后代结点的划分属性。

### 缺失值处理

**如何在属性值确实的情况下进行划分属性选择？**

根据没有缺失值的样本属性值的分布比例，为每一个样本赋予一个权重。

**给定划分属性，若样本在该属性上的值确实，如何对样本进行划分？**

若样本$x$在属性$a$上的值已知，将其划入相应的子结点，并赋予对应的权重$w_x$，对于权重未知，则将其划入所有子结点，并设置相应权重。

## 各种决策树之间的比较

### ID3算法

$ID3$算法的核心是在决策树各个结点上根据信息增益准则选择特征，递归构建决策树。

具体步骤：

1. 从根结点开始，对节点计算所有可能特征的信息增益，选择值最大的作为结点的特征；
2. 根据该特征的不同取值建立子结点，递归使用步骤1，构建决策树，直到所有特征的信息增益都很小或者没有特征可选择为止；

优点：结构简单，清晰易懂，方便灵活

缺点：分裂精度不高，不能处理缺省值，没有剪枝操作（无法应对过拟合），不能对连续值进行处理

### C4.5算法

$C4.5$是在$ID3$的基础上对上述三方面进行了优化：

* $C4.5$采用信息增益率作为属性选择依据；
* 可以对连续值属性、缺失值属性处理；
* $C4.5$采用后剪枝策略，防止过拟合；

### CART算法

具体步骤：

1. 设结点的训练集为$D$，计算现有特征对该数据集的基尼指数，对于每个特征$A$，其可能取的每个值$a$，根据样本点对$A=a$的测试，将数据集划分为两部分，并计算$A=a$的基尼指数；
2. 在所有肯能的特征$A$及所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点。依最优特征与最优切分点，从结点生成两个子结点，将数据集依特征划分到两个子结点中；
3. 递归上述1、2步骤；

